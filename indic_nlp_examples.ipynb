{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the _Indic NLP Library_ is to build Python based libraries for common text processing and Natural Language Processing in Indian languages. Indian languages share a lot of similarity in terms of script, phonology, language syntax, etc. and this library is an attempt to provide a general solution to very commonly required toolsets for Indian language text. \n",
    "\n",
    "The library provides the following functionalities: \n",
    "\n",
    "- Text Normalization\n",
    "- Tokenization\n",
    "- Sentence Splitter\n",
    "- Script Conversion\n",
    "- Romanization\n",
    "- Indicization\n",
    "- Script Information\n",
    "- Phonetic Similarity\n",
    "- Syllabification\n",
    "- Word Segmenation\n",
    "- Transliteration\n",
    "- Translation\n",
    "\n",
    "The data resources required by the Indic NLP Library are hosted in a different repository. These resources are required for some modules. You can download from the [Indic NLP Resources](https://github.com/anoopkunchukuttan/indic_nlp_resources) project.\n",
    "\n",
    "# Pre-requisites\n",
    "\n",
    "- Python 3.5+\n",
    "- [Morfessor 2.0 Python Library](http://www.cis.hut.fi/projects/morpho/morfessor2.shtml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**  ----- Set these variables ----- **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\aksha.LAPTOP-KKEAULMJ\\\\OneDrive\\\\Documents\\\\Python\\\\gitpro\\\\AI\\\\Chatbot\\\\bot\\\\bankBOT'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the local git repo for Indic NLP library\n",
    "INDIC_NLP_LIB_HOME=r\"\\C:\\Users\\aksha.LAPTOP-KKEAULMJ\\OneDrive\\Documents\\Python\\gitpro\\AI\\Chatbot\\indic\\indic_nlp_library\"\n",
    "\n",
    "# The path to the local git repo for Indic NLP Resources\n",
    "INDIC_NLP_RESOURCES=r\"C:\\Users\\aksha.LAPTOP-KKEAULMJ\\OneDrive\\Documents\\Python\\gitpro\\AI\\Chatbot\\indic\\indic_nlp_resources\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Library to Python path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'{}\\src'.format(INDIC_NLP_LIB_HOME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Export environment variable ** \n",
    "\n",
    "    export INDIC_RESOURCES_PATH=<path>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     OR\n",
    "     \n",
    "**set it programmatically**\n",
    "We will use that method for this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'indicnlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4435e3a58314>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mindicnlp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'indicnlp'"
     ]
    }
   ],
   "source": [
    "import indicnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'indicnlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-857d68b85ab1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mindicnlp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_resources_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mINDIC_NLP_RESOURCES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'indicnlp'"
     ]
    }
   ],
   "source": [
    "from indicnlp import common\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Initialize the Indic NLP library **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate rasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'indicnlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-5f49d51c6132>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mindicnlp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'indicnlp'"
     ]
    }
   ],
   "source": [
    "from indicnlp import loader\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Let's actually try out some of the API methods in the Indic NLP library **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the API functions require a language code. We use 2-letter ISO 639-1 codes. Some languages do not have assigned 2-letter codes. We use the following two-letter codes for such languages: \n",
    "\n",
    "- Konkani: kK\n",
    "- Manipuri: mP\n",
    "- Bodo: bD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples for various API features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization\n",
    "\n",
    "Text written in Indic scripts display a lot of quirky behaviour on account of varying input methods, multiple representations for the same character, etc. \n",
    "There is a need to canonicalize the representation of text so that NLP applications can handle the data in a consistent manner. The canonicalization primarily handles the following issues: \n",
    "\n",
    "    - Non-spacing characters like ZWJ/ZWNL\n",
    "    - Multiple representations of Nukta based characters \n",
    "    - Multiple representations of two part dependent vowel signs\n",
    "    - Typing inconsistencies: e.g. use of pipe (|) for poorna virama\n",
    "\n",
    "When data available is scarce, such normalization can help utilize the data more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "क़ क़\n",
      "\n",
      "Before normalization\n",
      "0x958 0x20 0x915 0x93c\n",
      "Length: 4\n",
      "\n",
      "After normalization\n",
      "0x915 0x93c 0x20 0x915 0x93c\n",
      "Length: 5\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "\n",
    "input_text=\"\\u0958 \\u0915\\u093c\"\n",
    "remove_nuktas=False\n",
    "factory=IndicNormalizerFactory()\n",
    "normalizer=factory.get_normalizer(\"hi\",remove_nuktas)\n",
    "output_text=normalizer.normalize(input_text)\n",
    "\n",
    "print(input_text)\n",
    "print()\n",
    "\n",
    "print('Before normalization')\n",
    "print(' '.join([ hex(ord(c)) for c in input_text ] ))\n",
    "print('Length: {}'.format(len(input_text)))\n",
    "print()    \n",
    "print('After normalization')\n",
    "print(' '.join([ hex(ord(c)) for c in output_text ] ))\n",
    "print('Length: {}'.format(len(output_text)))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Splitter\n",
    "\n",
    "A smart sentence splitter which uses a two-pass rule-based system to split the text into sentences. It knows of common prefixes in Indian languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "तो क्या विश्व कप 2019 में मैच का बॉस टॉस है?\n",
      "यानी मैच में हार-जीत में टॉस की भूमिका अहम है?\n",
      "आप ऐसा सोच सकते हैं।\n",
      "विश्वकप के अपने-अपने पहले मैच में बुरी तरह हारने वाली एशिया की दो टीमों पाकिस्तान और श्रीलंका के कप्तान ने हालांकि अपने हार के पीछे टॉस की दलील तो नहीं दी, लेकिन यह जरूर कहा था कि वह एक अहम टॉस हार गए थे।\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.tokenize import sentence_tokenize\n",
    "\n",
    "indic_string=\"\"\"तो क्या विश्व कप 2019 में मैच का बॉस टॉस है? यानी मैच में हार-जीत में \\\n",
    "टॉस की भूमिका अहम है? आप ऐसा सोच सकते हैं। विश्वकप के अपने-अपने पहले मैच में बुरी तरह हारने वाली एशिया की दो टीमों \\\n",
    "पाकिस्तान और श्रीलंका के कप्तान ने हालांकि अपने हार के पीछे टॉस की दलील तो नहीं दी, लेकिन यह जरूर कहा था कि वह एक अहम टॉस हार गए थे।\"\"\"\n",
    "sentences=sentence_tokenize.sentence_split(indic_string, lang='hi')\n",
    "for t in sentences:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization \n",
    "\n",
    "A trivial tokenizer which just tokenizes on the punctuation boundaries. This also includes punctuations for the Indian language scripts (the purna virama and the deergha virama). It returns a list of tokens.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String: सुनो, कुछ आवाज़ आ रही है। फोन?\n",
      "Tokens: \n",
      "सुनो\n",
      ",\n",
      "कुछ\n",
      "आवाज़\n",
      "आ\n",
      "रही\n",
      "है\n",
      "।\n",
      "फोन\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.tokenize import indic_tokenize  \n",
    "\n",
    "indic_string='सुनो, कुछ आवाज़ आ रही है। फोन?'\n",
    "\n",
    "print('Input String: {}'.format(indic_string))\n",
    "print('Tokens: ')\n",
    "for t in indic_tokenize.trivial_tokenize(indic_string): \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De-tokenization \n",
    "\n",
    "A de-tokenizer for Indian languages that can address punctuation in Indic languages. The de-tokenizer is useful when generating natural language output. It can be used as a post-processor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String: \" सुनो , कुछ आवाज़ आ रही है . \" , उसने कहा । \n",
      "Detokenized String: \"सुनो, कुछ आवाज़ आ रही है.\", उसने कहा। \n"
     ]
    }
   ],
   "source": [
    "from indicnlp.tokenize import indic_detokenize  \n",
    "indic_string='\" सुनो , कुछ आवाज़ आ रही है . \" , उसने कहा । '\n",
    "\n",
    "print('Input String: {}'.format(indic_string))\n",
    "print('Detokenized String: {}'.format(indic_detokenize.trivial_detokenize(indic_string,lang='hi')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Conversion\n",
    "\n",
    "Convert from one Indic script to another. This is a simple script which exploits the fact that Unicode points of various Indic scripts are at corresponding offsets from the base codepoint for that script. The following scripts are supported:\n",
    "\n",
    "_Devanagari (Hindi,Marathi,Sanskrit,Konkani,Sindhi,Nepali), Assamese, Bengali, Oriya, Gujarati, Gurumukhi (Punjabi), Sindhi, Tamil, Telugu, Kannada, Malayalam_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ராஜஸ்தாந\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.transliterate.unicode_transliterate import UnicodeIndicTransliterator\n",
    "input_text='राजस्थान'\n",
    "# input_text='രാജസ്ഥാന'\n",
    "# input_text='රාජස්ථාන'\n",
    "print(UnicodeIndicTransliterator.transliterate(input_text,\"hi\",\"ta\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Romanization\n",
    "\n",
    "Convert script text to Roman text in the ITRANS notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raajasthaana\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.transliterate.unicode_transliterate import ItransTransliterator\n",
    "\n",
    "input_text='राजस्थान'\n",
    "# input_text='ஆசிரியர்கள்'\n",
    "lang='hi'\n",
    "\n",
    "print(ItransTransliterator.to_itrans(input_text,lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicization (ITRANS to Indic Script)\n",
    "\n",
    "Let's call conversion of ITRANS-transliteration to an Indic script as **Indicization**!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "பித௣ந்\n",
      "baa\n",
      "bbf\n",
      "ba4\n",
      "be3\n",
      "ba8\n",
      "bcd\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.transliterate.unicode_transliterate import ItransTransliterator\n",
    "\n",
    "\n",
    "# input_text='rajasthAna'\n",
    "input_text='pitL^In'\n",
    "lang='hi'\n",
    "x=ItransTransliterator.from_itrans(input_text,lang)\n",
    "print(x)\n",
    "for y in x:\n",
    "    print('{:x}'.format(ord(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script Information\n",
    "\n",
    "Indic scripts have been designed keeping phonetic principles in nature and the design and organization of the scripts makes it easy to obtain phonetic information about the characters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Phonetic Feature Vector\n",
    "\n",
    "With each script character, a phontic feature vector is associated, which encodes the phontic properties of the character. This is a bit vector which is can be obtained as shown below:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from indicnlp.script import  indic_scripts as isc\n",
    "\n",
    "c='क'\n",
    "lang='hi'\n",
    "\n",
    "isc.get_phonetic_feature_vector(c,lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fields in this bit vector are (from left to right): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('basic_type', [0, 6]),\n",
       " ('vowel_length', [6, 8]),\n",
       " ('vowel_strength', [8, 11]),\n",
       " ('vowel_status', [11, 13]),\n",
       " ('consonant_type', [13, 18]),\n",
       " ('articulation_place', [18, 23]),\n",
       " ('aspiration', [23, 25]),\n",
       " ('voicing', [25, 27]),\n",
       " ('nasalization', [27, 29]),\n",
       " ('vowel_horizontal', [29, 32]),\n",
       " ('vowel_vertical', [32, 36]),\n",
       " ('vowel_roundness', [36, 38])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(isc.PV_PROP_RANGES.items(),key=lambda x:x[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the phonetic information database files in Indic NLP resources to know the definition of each of the bits. \n",
    "\n",
    "- _For Tamil Script_: [database](https://github.com/anoopkunchukuttan/indic_nlp_resources/blob/master/script/tamil_script_phonetic_data.csv) \n",
    "- _For other Indic Scripts_: [database](https://github.com/anoopkunchukuttan/indic_nlp_resources/blob/master/script/all_script_phonetic_data.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Phonetic Properties\n",
    "\n",
    "**Note:** _This interface below will be deprecated soon and a new interface will be available soon._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is vowel?:  False\n",
      "Is consonant?:  True\n",
      "Is velar?:  True\n",
      "Is palatal?:  False\n",
      "Is aspirated?:  False\n",
      "Is unvoiced?:  True\n",
      "Is nasal?:  False\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.langinfo import *\n",
    "\n",
    "c='क'\n",
    "lang='hi'\n",
    "\n",
    "print('Is vowel?:  {}'.format(is_vowel(c,lang)))\n",
    "print('Is consonant?:  {}'.format(is_consonant(c,lang)))\n",
    "print('Is velar?:  {}'.format(is_velar(c,lang)))\n",
    "print('Is palatal?:  {}'.format(is_palatal(c,lang)))\n",
    "print('Is aspirated?:  {}'.format(is_aspirated(c,lang)))\n",
    "print('Is unvoiced?:  {}'.format(is_unvoiced(c,lang)))\n",
    "print('Is nasal?:  {}'.format(is_nasal(c,lang)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Phonetic Similarity\n",
    "\n",
    "Using the phonetic feature vectors, we can define phonetic similarity between the characters (and underlying phonemes). The library implements some  measures for phonetic similarity between the characters (and underlying phonemes). These can be defined using the phonetic feature vectors discussed earlier, so users can implement additional similarity measures. \n",
    "\n",
    "The implemented similarity measures are: \n",
    "\n",
    "- cosine\n",
    "- dice\n",
    "- jaccard\n",
    "- dot_product\n",
    "- sim1 (Kunchukuttan _et al._, 2016)\n",
    "- softmax\n",
    "\n",
    "** References **\n",
    "\n",
    "Anoop Kunchukuttan, Pushpak Bhattacharyya, Mitesh Khapra. _Substring-based unsupervised transliteration with phonetic and contextual knowledge_. SIGNLL Conference on Computational Natural Language Learning ** (CoNLL 2016) **. 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between क and ख\n",
      "0.8333319444467593\n",
      "\n",
      "Similarity between क and भ\n",
      "0.4999991666680556\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.script import  indic_scripts as isc\n",
    "from indicnlp.script import  phonetic_sim as psim\n",
    "\n",
    "c1='क'\n",
    "c2='ख'\n",
    "c3='भ'\n",
    "lang='hi'\n",
    "\n",
    "print('Similarity between {} and {}'.format(c1,c2))\n",
    "print(psim.cosine(\n",
    "    isc.get_phonetic_feature_vector(c1,lang),\n",
    "    isc.get_phonetic_feature_vector(c2,lang)\n",
    "    ))\n",
    "\n",
    "print()\n",
    "\n",
    "print(u'Similarity between {} and {}'.format(c1,c3))\n",
    "print(psim.cosine(\n",
    "    isc.get_phonetic_feature_vector(c1,lang),\n",
    "    isc.get_phonetic_feature_vector(c3,lang)\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_You may have figured out that you can also compute similarities of characters belonging to different scripts._\n",
    "\n",
    "You can also get a similarity matrix which contains the similarities between all pairs of characters (within the same script or across scripts).\n",
    "\n",
    "Let's see how we can compare the characters across Devanagari and Malayalam scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between क and ഖ\n",
      "0.8333319444467593\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.script import  indic_scripts as isc\n",
    "from indicnlp.script import  phonetic_sim as psim\n",
    "\n",
    "\n",
    "slang='hi'\n",
    "tlang='ml'\n",
    "sim_mat=psim.create_similarity_matrix(psim.cosine,slang,tlang,normalize=False)\n",
    "\n",
    "c1='क'\n",
    "c2='ഖ'\n",
    "print('Similarity between {} and {}'.format(c1,c2))\n",
    "print(sim_mat[isc.get_offset(c1,slang),isc.get_offset(c2,tlang)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some similarity functions like `sim` do not generate values in the range [0,1] and it may be more convenient to have the similarity values in the range [0,1]. This can be achieved by setting the `normalize` paramter to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between क and ഖ\n",
      "0.06860894001932027\n"
     ]
    }
   ],
   "source": [
    "slang='hi'\n",
    "tlang='ml'\n",
    "sim_mat=psim.create_similarity_matrix(psim.sim1,slang,tlang,normalize=True)\n",
    "\n",
    "c1='क'\n",
    "c2='ഖ'\n",
    "print(u'Similarity between {} and {}'.format(c1,c2))\n",
    "print(sim_mat[isc.get_offset(c1,slang),isc.get_offset(c2,tlang)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi string: पिछले दिनों हम लोगों ने कई उत्सव मनाये. कल, हिन्दुस्तान भर में श्री कृष्ण जन्म-महोत्सव मनाया गया.\n",
      "gu string: वीतेला दिवसोमां आपणे केटलाय उत्सवो उजव्या. हजी गइकाले ज पूरा हिंदुस्तानमां श्रीकृष्ण जन्मोत्सव उजववामां आव्यो.\n",
      "Both strings are shown in Devanagari script using script conversion for readability.\n",
      "LCSR: 0.5545454545454546\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.script import  indic_scripts as isc\n",
    "from indicnlp.transliterate.unicode_transliterate import UnicodeIndicTransliterator\n",
    "\n",
    "lang1_str='पिछले दिनों हम लोगों ने कई उत्सव मनाये. कल, हिन्दुस्तान भर में श्री कृष्ण जन्म-महोत्सव मनाया गया.'\n",
    "lang2_str='વીતેલા દિવસોમાં આપણે કેટલાય ઉત્સવો ઉજવ્યા. હજી ગઇકાલે જ પૂરા હિંદુસ્તાનમાં શ્રીકૃષ્ણ જન્મોત્સવ ઉજવવામાં આવ્યો.'\n",
    "lang1='hi'\n",
    "lang2='gu'\n",
    "\n",
    "lcsr, len1, len2 = isc.lcsr_indic(lang1_str,lang2_str,lang1,lang2)\n",
    "\n",
    "print('{} string: {}'.format(lang1, lang1_str))\n",
    "print('{} string: {}'.format(lang2, UnicodeIndicTransliterator.transliterate(lang2_str,lang2,lang1)))\n",
    "print('Both strings are shown in Devanagari script using script conversion for readability.')\n",
    "print('LCSR: {}'.format(lcsr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthographic Syllabification\n",
    "\n",
    "_Orthographic Syllabification_ is an approximate syllabification process for Indic scripts, where CV+ units are defined to be _orthographic syllables_.\n",
    "\n",
    "See the following paper for details:\n",
    "\n",
    "Anoop Kunchukuttan, Pushpak Bhattacharyya. [_Orthographic Syllable as basic unit for SMT between Related Languages_](https://arxiv.org/abs/1610.00634). Conference on Empirical Methods in Natural Language Processing **(EMNLP 2016)**. 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ज ग दी श च ंद्र\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.syllable import  syllabifier\n",
    "\n",
    "w='जगदीशचंद्र'\n",
    "lang='hi'\n",
    "\n",
    "print(' '.join(syllabifier.orthographic_syllabify(w,lang)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Segmentation\n",
    "\n",
    "Unsupervised morphological analysers for various Indian language. Given a word, the analyzer returns the componenent morphemes. \n",
    "The analyzer can recognize inflectional and derivational morphemes. \n",
    "\n",
    "The following languages are supported:\n",
    "\n",
    "_Hindi, Punjabi, Marathi, Konkani, Gujarati, Bengali, Kannada, Tamil, Telugu, Malayalam_\n",
    "\n",
    "Support for more languages will be added soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.morph import unsupervised_morph \n",
    "from indicnlp import common\n",
    "\n",
    "analyzer=unsupervised_morph.UnsupervisedMorphAnalyzer('mr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "आपल्या\n",
      "हिरड्या\n",
      "ंच्या\n",
      "आणि\n",
      "दाता\n",
      "ंच्या\n",
      "मध्ये\n",
      "जीवाणू\n",
      "असतात\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "indic_string='आपल्या हिरड्यांच्या आणि दातांच्यामध्ये जीवाणू असतात .'\n",
    "\n",
    "analyzes_tokens=analyzer.morph_analyze_document(indic_string.split(' '))\n",
    "\n",
    "for w in analyzes_tokens: \n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transliteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [_BrahmiNet_](http://www.cfilt.iitb.ac.in/brahminet/static/rest.html) REST API for transliteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.cfilt.iitb.ac.in/indicnlpweb/indicnlpws/transliterate_bulk/en/hi/manish%20joe/rule\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hi': ['मनिश् जोए']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from urllib.parse import  quote\n",
    "\n",
    "text=quote('manish joe')\n",
    "# text=quote('मनिश् जोए')\n",
    "url='http://www.cfilt.iitb.ac.in/indicnlpweb/indicnlpws/transliterate_bulk/en/hi/{}/statistical'.format(text)\n",
    "print(url)\n",
    "response = requests.get(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use _BrahmiNet_ through [this](http://www.cfilt.iitb.ac.in/brahminet) web interface.  \n",
    "\n",
    "You can read more about _BrahmiNet_ [here](http://www.cfilt.iitb.ac.in/brahminet/static/publications/brahminet_naacl2015.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acronyms\n",
    "\n",
    "Acronyms have a different behaviour while transliterating. Hence, a rule-based transliterator for transliterating English acronyms to Indian languages is available. \n",
    "\n",
    "This can also be used to generate synthetic transliteration data to train a Indian language to English transliterator for acronyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'आईसीआईसीआई'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from indicnlp.transliterate import acronym_transliterator\n",
    "\n",
    "ack_transliterator=acronym_transliterator.LatinToIndicAcronymTransliterator()\n",
    "ack_transliterator.transliterate('ICICI',lang='hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [_Shata-anuvaadak_](http://www.cfmilt.iitb.ac.in/indic-translator) for translation.\n",
    "You can read more about _Shata-anuvaadak_ [here](http://www.lrec-conf.org/proceedings/lrec2014/pdf/414_Paper.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.cfilt.iitb.ac.in/indicnlpweb/indicnlpws/translate/en/mr/Mumbai%20is%20the%20capital%20of%20Maharashtra/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mr': 'राजधानी महाराष्ट्र मुंबई आहे . '}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from urllib.parse import  quote\n",
    "\n",
    "text=quote('Mumbai is the capital of Maharashtra')\n",
    "# text=quote('मनिश् जोए')\n",
    "url='http://www.cfilt.iitb.ac.in/indicnlpweb/indicnlpws/translate/en/mr/{}/'.format(text)\n",
    "## Note the forward slash '/' at the end of the URL. It's should be there, but please live with it for now!\n",
    "\n",
    "print(url)\n",
    "response = requests.get(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "184px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
